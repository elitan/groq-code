<div align="center">
  <h1>Groq Python</h1>
</div>

<br />

What can you build when LLM models are extremly smart and inference is extremly fast?

Here's an example of the current state of the art in LLM models and the current state of the art in inference speed.

- **LLM:** [LLaMA 3 from Meta](https://llama.meta.com/llama3/).
- **Inference:** [LPU Inference Engine from Groq](https://groq.com/).
